{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of Wav2Vec2.0 algorithm using HuggingFace library, and computation of WER on our Kaggle Dataset"
      ],
      "metadata": {
        "id": "n0I9Gslk05Mt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install libraries"
      ],
      "metadata": {
        "id": "tMcYJXXg1CrF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAjcp1Vj0EOC"
      },
      "outputs": [],
      "source": [
        "pip install huggingsound"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.utils import plot_model\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Input, Layer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, add, Concatenate, Reshape, concatenate, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "NERt19xm03bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Wav2Vec2.0 model"
      ],
      "metadata": {
        "id": "ZKVHKp001Fxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingsound import SpeechRecognitionModel\n",
        "\n",
        "model = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-french\")"
      ],
      "metadata": {
        "id": "oGFXE6oj0Hnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "XNqWagcL0J5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test on an audio, coming from a radio programme\n",
        "audio_paths = [\"/content/radiomp3_court.mp3\"]\n",
        "\n",
        "transcriptions = model.transcribe(audio_paths)[0]\n",
        "print(transcriptions[\"transcription\"]) #return the text = transcription, strart_timestamps, end_timestamps, probabilities"
      ],
      "metadata": {
        "id": "4n1gyhvD0L1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing Wav2Vec on our dataset"
      ],
      "metadata": {
        "id": "da6MF2Zs1fE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#utilsation des donn√©es de notre dataset\n",
        "liste_lesmis = os.listdir(\"/content/lesmis\")\n",
        "liste_lupin = os.listdir(\"/content/lupincontresholme\")\n",
        "print(liste_lesmis[:10])"
      ],
      "metadata": {
        "id": "tqb9sLFL0NPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_path = [f'/content/lesmis/{liste_lesmis[0]}']\n",
        "#print(audio_path)\n",
        "transcriptions = model.transcribe(audio_path)[0]\n",
        "print(transcriptions[\"transcription\"])"
      ],
      "metadata": {
        "id": "we5NtMX50XhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 300 #take 300 elements of each books to transcript\n",
        "res = {}\n",
        "for i in range(N):\n",
        "    index = random.randint(0, len(liste_lesmis)-1)\n",
        "    audio_path = [f'/content/lesmis/{liste_lesmis[index]}']\n",
        "    transcriptions = model.transcribe(audio_path)[0]\n",
        "    texte = transcriptions[\"transcription\"]\n",
        "    res[liste_lesmis[index]] = texte\n",
        "\n",
        "for i in range(N):\n",
        "    index = random.randint(0, len(liste_lupin)-1)\n",
        "    audio_path = [f'/content/lupincontresholme/{liste_lupin[index]}']\n",
        "    transcriptions = model.transcribe(audio_path)[0]\n",
        "    texte = transcriptions[\"transcription\"]\n",
        "    res[liste_lupin[index]] = texte"
      ],
      "metadata": {
        "id": "VAOZTSXH0ZBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list(res.items()), columns=['filename', 'Texte'])"
      ],
      "metadata": {
        "id": "Ye3jifZ30apA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save if needed\n",
        "df.to_csv('transcriptWavToVeq.csv')\n",
        "files.download('transcriptWavToVeq.csv')"
      ],
      "metadata": {
        "id": "J9F7GKHI0dU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_reel = pd.read_csv('/content/transcript.txt', sep=\"|\", header=None)\n",
        "df_reel.columns = [\"filename\", \"transcript1\", \"transcript2\", \"duration\"]"
      ],
      "metadata": {
        "id": "LZvsfAn70e9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_reel = df_reel.iloc[:,:2]\n",
        "df_reel"
      ],
      "metadata": {
        "id": "L7G32VxM0g6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#need to remove punctuation in real transcripts to better make comparisons\n",
        "\n",
        "punct = list(string.punctuation)\n",
        "punct.pop(12) #pour conserver les tirets\n",
        "#df_reel[\"transcript1\"] = df_reel[\"transcript1\"].apply(lambda x: x.replace(\"'\",\"\")) #remove apostrophe et concatenate\n",
        "for pun in punct :\n",
        "    df_reel[\"transcript1\"] = df_reel[\"transcript1\"].apply(lambda x: x.replace(pun,\"\")) #remove ponctuation\n",
        "df_reel[\"transcript1\"] = df_reel[\"transcript1\"].apply(lambda x: x.lower()) #remove Upercase\n",
        "df_reel[\"transcript1\"] = df_reel[\"transcript1\"].apply(lambda x: x.replace(\"\\s+\",\" \")) #remove double space\n",
        "df_reel.head()"
      ],
      "metadata": {
        "id": "-TZGzAU50iG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "MJPrdN260j-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_W2V = pd.read_csv('/content/transcriptWavToVeq.csv')"
      ],
      "metadata": {
        "id": "t5Qr-1_t0l-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_W2V"
      ],
      "metadata": {
        "id": "-eYmkWFH0nir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refs = []\n",
        "hyps= []\n",
        "for i in range (df_W2V.shape[0]):\n",
        "  filename = df_W2V[\"filename\"].iloc[i]\n",
        "  texte_W2V = df_W2V[\"Texte\"].iloc[i]\n",
        "  hyps.append(texte_W2V)\n",
        "  #print(filename)\n",
        "  #print(texte_W2V)\n",
        "  lien = filename[:-9]\n",
        "  filename_reel = lien+\"/\"+filename\n",
        "  #print(filename_reel)\n",
        "  texte_reel = df_reel.loc[df_reel.filename==filename_reel]\n",
        "  texte_reel = texte_reel.iloc[0, 1]\n",
        "  refs.append(texte_reel)\n",
        "  #print(texte_reel)"
      ],
      "metadata": {
        "id": "ZXFhxE370o9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definition of the WER, between the reference and or hypothesis"
      ],
      "metadata": {
        "id": "kVDeKJ8715iO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wer(ref, hyp ,debug=True):\n",
        "    r = ref.split()\n",
        "    h = hyp.split()\n",
        "    #costs will holds the costs, like in the Levenshtein distance algorithm\n",
        "    costs = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
        "    # backtrace will hold the operations we've done.\n",
        "    # so we could later backtrace, like the WER algorithm requires us to.\n",
        "    backtrace = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
        " \n",
        "    OP_OK = 0\n",
        "    OP_SUB = 1\n",
        "    OP_INS = 2\n",
        "    OP_DEL = 3\n",
        "    DEL_PENALTY = 1\n",
        "    INS_PENALTY = 1\n",
        "    SUB_PENALTY = 1\n",
        "    \n",
        "    # First column represents the case where we achieve zero\n",
        "    # hypothesis words by deleting all reference words.\n",
        "    for i in range(1, len(r)+1):\n",
        "        costs[i][0] = DEL_PENALTY*i\n",
        "        backtrace[i][0] = OP_DEL\n",
        "    \n",
        "    # First row represents the case where we achieve the hypothesis\n",
        "    # by inserting all hypothesis words into a zero-length reference.\n",
        "    for j in range(1, len(h) + 1):\n",
        "        costs[0][j] = INS_PENALTY * j\n",
        "        backtrace[0][j] = OP_INS\n",
        "    \n",
        "    # computation\n",
        "    for i in range(1, len(r)+1):\n",
        "        for j in range(1, len(h)+1):\n",
        "            if r[i-1] == h[j-1]:\n",
        "                costs[i][j] = costs[i-1][j-1]\n",
        "                backtrace[i][j] = OP_OK\n",
        "            else:\n",
        "                substitutionCost = costs[i-1][j-1] + SUB_PENALTY # penalty is always 1\n",
        "                insertionCost    = costs[i][j-1] + INS_PENALTY   # penalty is always 1\n",
        "                deletionCost     = costs[i-1][j] + DEL_PENALTY   # penalty is always 1\n",
        "                 \n",
        "                costs[i][j] = min(substitutionCost, insertionCost, deletionCost)\n",
        "                if costs[i][j] == substitutionCost:\n",
        "                    backtrace[i][j] = OP_SUB\n",
        "                elif costs[i][j] == insertionCost:\n",
        "                    backtrace[i][j] = OP_INS\n",
        "                else:\n",
        "                    backtrace[i][j] = OP_DEL\n",
        "                 \n",
        "    # back trace though the best route:\n",
        "    i = len(r)\n",
        "    j = len(h)\n",
        "    numSub = 0\n",
        "    numDel = 0\n",
        "    numIns = 0\n",
        "    numCor = 0\n",
        "    if debug:\n",
        "        print(\"OP\\tREF\\tHYP\")\n",
        "        lines = []\n",
        "    while i > 0 or j > 0:\n",
        "        if backtrace[i][j] == OP_OK:\n",
        "            numCor += 1\n",
        "            i-=1\n",
        "            j-=1\n",
        "            if debug:\n",
        "                lines.append(\"OK\\t\" + r[i]+\"\\t\"+h[j])\n",
        "        elif backtrace[i][j] == OP_SUB:\n",
        "            numSub +=1\n",
        "            i-=1\n",
        "            j-=1\n",
        "            if debug:\n",
        "                lines.append(\"SUB\\t\" + r[i]+\"\\t\"+h[j])\n",
        "        elif backtrace[i][j] == OP_INS:\n",
        "            numIns += 1\n",
        "            j-=1\n",
        "            if debug:\n",
        "                lines.append(\"INS\\t\" + \"****\" + \"\\t\" + h[j])\n",
        "        elif backtrace[i][j] == OP_DEL:\n",
        "            numDel += 1\n",
        "            i-=1\n",
        "            if debug:\n",
        "                lines.append(\"DEL\\t\" + r[i]+\"\\t\"+\"****\")\n",
        "    if debug:\n",
        "        lines = reversed(lines)\n",
        "        for line in lines:\n",
        "            print(line)\n",
        "        print(\"#cor \" + str(numCor))\n",
        "        print(\"#sub \" + str(numSub))\n",
        "        print(\"#del \" + str(numDel))\n",
        "        print(\"#ins \" + str(numIns))\n",
        "    wer_result = round( (numSub + numDel + numIns) / (float) (len(r)), 3)\n",
        "    return {'WER':wer_result, 'numCor':numCor, 'numSub':numSub, 'numIns':numIns, 'numDel':numDel, \"numCount\": len(r)}\n",
        "\n",
        "def wers(refs, hyps):\n",
        "    numSub = 0\n",
        "    numDel = 0\n",
        "    numCor = 0\n",
        "    numIns = 0\n",
        "    numCount = 0\n",
        "    for ref, hyp in zip(refs, hyps):\n",
        "        result = wer(ref, hyp, False)\n",
        "        numSub += result[\"numSub\"]\n",
        "        numDel += result[\"numDel\"]\n",
        "        numCor += result[\"numCor\"]\n",
        "        numIns += result[\"numIns\"]\n",
        "        numCount += result[\"numCount\"]\n",
        " \n",
        "    return round( (numSub + numDel + numIns) / (float) (numCount), 3)"
      ],
      "metadata": {
        "id": "V2W6rXss0rFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wer(texte_reel, texte_W2V ,debug=True))"
      ],
      "metadata": {
        "id": "bNEE6cYm0shL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "aD3B8xRI2Aaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wers(refs, hyps))"
      ],
      "metadata": {
        "id": "ZLdIH-4F0ubh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}